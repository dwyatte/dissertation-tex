\documentclass[dwyatte_dissertation.tex]{subfiles} 
\begin{document}

\chapter{Neural model of spatiotemporal prediction for object recognition}

\section{Introduction}
TODO

\section{Methods}

\subsection{Model architecture}

The model consisted of four primary layers (Figure \ref{fig:v1_v2}) whose parameters are described in detail here. Two of the layers contained columnar structure necessary for learning using the LeabraTI framework (Chapter \ref{chap:leabrati}):

% task fig
\begin{figure}[h!]
\begin{center}
\end{center}
\caption{}{}
\label{fig:v1_v2}
\end{figure}

\textbf{Retina and V1 preprocessing:} Input was provided to the model via a 24x24 topographic filter bank that preprocessed images offline from the model proper. This preprocessing step is consistent with a large class of biological models describing object recognition in cortex \cite[e.g.,]{RiesenhuberPoggio99,OReillyWyatteHerdEtAl13} and in the case of the present model, represents visual processing from the level of the retina through V1 simple cells \cite{HubelWiesel62} Grayscale bitmap images were scaled to 24x24 pixels and convolved with Gabor filters at four orientations (0$^\circ$, 45$^\circ$, 90$^\circ$, and 135$^\circ$) at two polarities (off-on and on-off) producing a 24x24x4x2 set of inputs. Each Gabor filter was implemented as 6x6 pixel kernel, with a wavelength $\lambda$ = 6 and Gaussian width terms of $\sigma_x$ = 1.8 and $\sigma_y$ = 1.2. A static nonlinearity was applied to the output of the filtering step in the form of a simplified \textit{k}-Winners-Take-All (\textit{k}WTA) inhibitory competition function that reduced activation across the 4x2 filter bank to the equivalent of \textit{k} = 1 fully active units \cite[see][Supporting Information]{OReillyWyatteHerdEtAl13}.

\textbf{Early visual columns:} 24x24 topographic layer arranged into groups of 4x2 units (4608 total units), decomposed into superficial (Layers 2 and 3) and deep (Layer 6) neuron subtypes. Each superficial unit receives the output of the retina/V1 preprocessing step. \textit{k}WTA inhibition for superficial units was set to 60\% of the average of the top \textit{k} active units compared to the average of all other superficial units with each 4x2 unit group using a value of \textit{k} = 2. Deep units received from 5x5 columns of superficial units (200 inputs per deep unit) integrated into a single value that was used as the additional context input channel for each superficial unit.

\textbf{Secondary visual columns:} 6x6 topographic layer arranged into groups of 7x7 units (1764 total units), also decomposed into superficial and deep neuron subtypes. Each superficial unit receives from 8x8 topographical neighborhoods of early visual columns (512 afferents per superficial unit) and sends back reciprocal connections with the same topography. \textit{k}WTA for superficial units was set to 60\% of the average of the top \textit{k} active units compared to the average of all other superficial units with each and 15\% activity within each unit group. Deep units received from 3x3 columns of superficial units (441 inputs per deep unit) integrated into a single value that was used as the additional context input channel for each superficial unit.

\textbf{Output units:} 10 x 10 layer without unit group or columnar substructure (100 total units). Each unit receives a full projection from secondary visual columns (1764 afferents per unit) and fully projects back to all columns. A \textit{k}WTA value of \textit{k} = 1 was used to enforce a localist representation. The localist representation is a computational simplification that allowed an identity readout of lower-level features similar to that provided by inferotemporal (IT) neurons in cortex albeit using a population code \cite{HungKreimanPoggioEtAl05,LiCoxZoccolanEtAl09}.


\subsection{LeabraTI learning rule}

\subsection{Input environment}

\section{Results}

\section{Discussion}

\bibliographystyle{apa}
\bibliography{ccnlab}
\end{document}