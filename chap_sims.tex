\documentclass[dwyatte_dissertation.tex]{subfiles} 
\begin{document}

\chapter{Neural model of spatiotemporal prediction for object recognition}

\section{Introduction}
TODO

\section{Methods}

\subsection{Model architecture}

The model consisted of four layers (Figure \ref{fig:v1_v2}) whose parameters are described in detail in the following paragraphs. Two of the layers contained columnar substructure which was necessary for implementing learning using the LeabraTI framework (Chapter \ref{chap:leabrati}).

% task fig
\begin{figure}[h!]
\begin{center}
\end{center}
\caption{}{}
\label{fig:v1_v2}
\end{figure}

\textbf{Retina and V1 preprocessing:} Input was provided to the model via a 24x24 topographic filter bank that preprocessed images offline from the model proper. This preprocessing step is consistent with a large class of biological models describing object recognition in cortex \cite[e.g.,]{RiesenhuberPoggio99,SerreOlivaPoggio07,OReillyWyatteHerdEtAl13} and in the case of the present model, represents visual processing from the level of the retina through V1 simple cells \cite{HubelWiesel62}. Grayscale bitmap images were scaled to 24x24 pixels and convolved with Gabor filters at four orientations (0$^\circ$, 45$^\circ$, 90$^\circ$, and 135$^\circ$) and two polarities (off-on and on-off) producing a 24x24x4x2 set of inputs. Each Gabor filter was implemented as 6x6 pixel kernel, with a wavelength $\lambda$ = 6 and Gaussian width terms of $\sigma_x$ = 1.8 and $\sigma_y$ = 1.2. A static nonlinearity was applied to the output of the filtering step in the form of a modified \textit{k}-Winners-Take-All (\textit{k}WTA) inhibitory competition function that reduced activation across the 4x2 filter bank to the equivalent of \textit{k} = 1 fully active units \cite[see][Supporting Information]{OReillyWyatteHerdEtAl13}.

\textbf{Primary visual layers:} 24x24 topographic layer arranged into groups of 4x2 units (4608 total units), decomposed into superficial (Layers 2 and 3) and deep (Layer 6) neuron subtypes. Each superficial unit received the output of the retina/V1 preprocessing step. \textit{k}WTA inhibition for superficial units was set to 60\% of the average of the top \textit{k} active units compared to the average of all other superficial units with each 4x2 unit group using a value of \textit{k} = 2. Deep units received from 5x5 columns of superficial units (200 inputs per deep unit) integrated into a single value that was used as the additional context input channel for each superficial unit.

\textbf{Secondary visual layers:} 6x6 topographic layer arranged into groups of 7x7 units (1764 total units), also decomposed into superficial and deep neuron subtypes. Each superficial unit received from 8x8 topographical neighborhoods of early visual columns (512 afferents per unit) and sent back reciprocal connections with the same topography. \textit{k}WTA for superficial units was set to 60\% of the average of the top \textit{k} active units compared to the average of all other superficial units with each and 15\% activity within each unit group. Deep units received from 3x3 columns of superficial units (441 inputs per deep unit) integrated into a single value that was used as the additional context input channel for each superficial unit.

\textbf{Output layer:} 10x10 layer (100 total units) without unit group or columnar substructure. Each unit received a full projection from secondary visual columns (1764 afferents per unit) and fully projected back to all columns. A \textit{k}WTA value of \textit{k} = 1 was used to enforce a localist representation. The localist representation is a computational simplification that allowed an identity readout of lower-level features without population decoding similar to that provided by inferior temporal (IT) neurons  \cite{HungKreimanPoggioEtAl05,LiCoxZoccolanEtAl09}. 

\subsection{LeabraTI learning algorithm}

LeabraTI was implemented as an extension of the standard Leabra algorithm which is described in detail in \incite{OReillyMunakata00} and \incite{OReillyMunakataFrankEtAl12}. Standard Leabra learning operates across two phases: a \textit{minus} phase that represents the system's expectation for a given input and a \textit{plus} phase, representing observation of the outcome. The difference between the minus and plus phases, along with additional self-organizing mechanisms, is used in computing the synaptic weight update function at the end of each plus phase. 

LeabraTI extends standard Leabra learning by interleaving its minus and plus phases over temporally contiguous input sequences. In standard Leabra, the minus phase depends on clamped inputs from the sensory periphery to drive the expectation while the plus phase uses clamped outputs from other neural systems to drive the outcome. In LeabraTI, the minus phase expectation is not driven by the sensory periphery, but instead by context represented by Layer 6 neurons. During this period, driving potential from the sensory periphery is suppressed by the inhibitory properties of the alpha rhythm (\nopcite{KlimeschSausengHanslmayr07}; \abbrevnopcite{MathewsonLlerasBeckEtAl11}). During the plus phase, alpha disinhibition shifts driving potential back to the sensory periphery. Layer 6 neurons' context is also updated after each plus phase by the 10 Hz bursting of Layer 5b neurons (\nopcite{ConnorsGutnickPrince82,SilvaAmitaiConnors91}; \abbrevnopcite{FranceschettiGuatteoPanzicaEtAl95}). 

One consequence of LeabraTI's temporal interleaving of minus and plus is that the initial minus phase in an input sequence does not have access to the previous moment's stored context. Even if there was stored context available, it would represent information from a previous prior, possibly unrelated input sequence. To address this, weight updates are prevented for the first minus-plus phase pair, and enabled thereafter. Presumably this process is facilitated by a neural mechanism that is sensitive to the repetition of inputs over time such as acetylcholine \cite{ThielHensonMorrisEtAl01,ThielHensonDolan02}

LeabraTI is only used to update the synaptic weights between superficial and deep synapses. Inter-areal feedforward and feedback projections bifurcate from the local column, directly synapsing disparate populations of superficial neurons and thus weight updates in these cases are handled by standard Leabra equations. In computing the difference between the plus and minus phases, standard Leabra uses the rate of sending (\textit{x}) and receiving (\textit{y}) neurons:

\begin{align*}
\Delta w_{i,j} = f_{Leabra}(x^+y^+ - x^-y^-)
\end{align*}

Because Layer 6 neurons are only active during the minus phase, LeabraTI uses the full column's rate in computing the plus phase activity. In the brain, this weight update likely operates on the Layer 5 $\rightarrow$ Layer 6 synapses, but LeabraTI models this synapse as a perfect one-to-one relay, and so the plastic superficial-to-deep projection is used instead:

%\begin{align*}
%\Delta w_{i,j} = f_{LeabraTI}(super^+ - super^-deep^-)
%\end{align*}

% equations
% like an SRN but rotated so that the context only needs to be pre-computed
% which prjns use ti
%
% first trial unlearnable flag

% context filtered through learned synaptic weights
% stable cx-thal wts

\subsection{Training and testing environment}

\section{Results}

\section{Discussion}

\bibliographystyle{apa}
\bibliography{ccnlab}
\end{document}