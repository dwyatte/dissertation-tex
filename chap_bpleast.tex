\documentclass[dwyatte_dissertation.tex]{subfiles} 
\begin{document}

\chapter{Effects of spatial and temporal prediction during prolonged learning of novel objects}
\label{chap:bpleast}

\section{Introduction}
TODO

\section{Methods}

\subsection{Participants}
A total of 62 students from the University of Colorado Boulder participated in the experiment (ages 18-22, mean=19.11 years; 30 male, 32 female). All participants reported normal or corrected-to-normal vision and received course credit as	compensation for their participation. Informed consent was obtained from each participant prior to the experiment in accordance with Institutional Review Board policy at the University of Colorado.

\subsection{Stimuli}
Novel ``paper clip'' objects were used as stimuli (see Chapter \ref{chap:pleast} Methods). A total of eight objects were used -- four as targets and four as distractors. The four target objects were also used in the Chapter \ref{chap:pleast} experiment. Target and distractor objects were paired together for the purposes of the experiment. All objects are shown in Figure \ref{fig:bpleast_objs}.

% paperclip targets fig
\begin{figure}[h!]
\textbf{A} \\
\includegraphics[width=160mm]{figs/chap_bpleast/paperclip_targets.pdf} \\
\textbf{B} \\
\includegraphics[width=160mm]{figs/chap_bpleast/paperclip_distractors.pdf} \\
\caption{Novel ``paper clip'' objects}{Four target (\textbf{A}) and four distractor object pairs (\textbf{B}) used in the experiment. See Chapter \ref{chap:pleast} Methods for additional information.}
\label{fig:bpleast_objs}
\end{figure}

\subsection{Procedure}
The experiment was divided into 16 blocks, each containing a training period followed by a series of test trials (Figure \ref{fig:bpleast_task}). During the training period of a given block, participants observed one of the target objects rotate about its y-axis. The object either rotated coherently (i.e., spatially predictable, S+ conditions) or in a random manner (S- conditions). Coherent rotation was composed of adjacent views spaced 12 degrees apart. The object made four complete rotations during the study period. All views of the object were still presented four times each in the random case. The presentation rate during the study period was either 10 Hz with a 50 ms on time and 50 ms off time (i.e., temporally predictable, T+ conditions) or variable with a 50 ms on time and off times ranging from 16.67-400 ms (T- conditions). 

The S+/- and T+/- conditions were crossed and each of the target-distractor object pairs was assigned to one of the four conditions. These assignments were approximately counterbalanced across participants (Assignment 1: \textit{N}=15; Assignment 2: \textit{N}=17; Assignment 3: \textit{N}=15; Assignment 4: \textit{N}=15). Each block condition with its target-distractor pairing was repeated for four blocks during the experiment. Block order randomized was randomized for each participant.

During each block, participants were instructed to study the target object during the training period and then complete a series of 30 test trials. On each test trial, either the target object or its paired distractor was presented. Participants were instructed to respond ``same'' if they believed the object depicted the trained target object or ``different'' if they believed it depicted the distractor object. Half of the test trials contained 15 views of the target object spaced 24 degrees apart, and the other half contained 15 views of the distractor, also spaced 24 degrees apart. Test trials were shown in a random order and feedback was withheld to prevent participants from changing their response criteria over the course of a block. 

The experiment was displayed on an LCD monitor at native resolution operating at 60 Hz using the Psychophysics Toolbox Version 3 \cite{Brainard97,Pelli97}. All stimuli were presented on an isoluminant 50\% gray background and subtended approximately 5 degrees of visual angle. Test trials began with a fixation cross (200 ms) followed by a blank (400 ms) followed by the probe stimulus (100 ms). Participants were required to respond within 2000 ms. Subsquent test trials were separated by a variable intertrial interval of 1000-1400 ms.

The experiment began with a practice block to ensure that participants understood the task. The training period during the practice block was always spatially and temporally predictable and used a reserved target object and distractor that were not further used in any of the experimental blocks. During the practice test trials, participants received feedback after responding according to whether they were correct or incorrect. After completing the practice block, participants were informed that future training periods could be presented in spatially and/or temporally unpredictable manners.

% paperclip targets fig
\begin{figure}[h!]
\includegraphics[width=160mm]{figs/chap_bpleast/paperclip_task.pdf} \\
\caption{Experimental procedure}{Experimental trials were composed of a training period followed by a testing period. The training period depicted a target object rotating a total of four times around its vertical axis. Rotation was either spatially and temporally predictable, spatially predictable or temporally predictable only, or completely unpredictable. The test period contained 30 trials that depicted either the training object or its paired distractor at 15 viewing angles each.}
\label{fig:bpleast_task}
\end{figure}

\section{Results}
% timeouts removed from data
Three subjects were excluded from behavioral analysis for accuracy 2.7$\sigma$ (or further) below mean accuracy across subjects. All three excluded subjects were assigned condition-object 3 resulting in the final counterbalancing -- Assignment 1: \textit{N}=15; Assignment 2: \textit{N}=14; Assignment 3: \textit{N}=15; Assignment 4: \textit{N}=15. The remaining 59 subjects were submitted to a 2x2 ANOVA with spatial and temporal predictability as within-subjects factors and counterbalancing assignment as a between-subjects factor. Accuracy and reaction times are plotted in Figure \ref{fig:bpleast_behave}. Transformed behavioral measures (e.g., \textit{d'}, inverse efficiency; see Chapter \ref{chap:pleast} Results) showed similar patterns to the raw measures and were thus omitted from analysis.

% oh, behave
\begin{figure}[h!]
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=80mm]{figs/chap_bpleast/results_accuracy.pdf} & 
\includegraphics[width=80mm]{figs/chap_bpleast/results_rt.pdf} \\
\end{tabular}
\end{center}
\caption{Behavioral measures of spatial and temporal predictability}{Accuracy and reaction time as a function of predictability during the training period. S-/+ refers to spatially unpredictable and predictable, T-/+ to temporally unpredictable and predictable. Error bars depict within-subjects error using the method described in \protect\incite{Cousineau05} adapted for standard error.}
\label{fig:bpleast_behave}
\end{figure}

% Acc
% Assignment: F(1, 57) = 0.156, p = 0.694
% Spatial: F(1, 57) = 4.496, p = 0.0383 * 
% Temporal: F(1, 57) = 4.149, p = 0.0463 * 
% Int: F(1, 57) = 0.197, p = 0.659

Overall, subjects were less accurate when the training period was spatially predictable (\textit{F}(1, 57) = 4.50, \textit{p} = 0.038) or temporally predictable (\textit{F}(1, 57) = 4.20, \textit{p} = 0.046). The interaction between spatial and temporal predicability failed to reach significance (\textit{F}(1, 57) = 0.20, \textit{p} = 0.659). Subjects were least accurate for the combined spatial and temporal predictability condition (denoted S+T+ in Figure \ref{fig:bpleast_behave}). This condition significantly differed from the completely unpredictable condition (S-T-) (\textit{t}(58) = -2.8587, \textit{p} = 0.001), and trended toward significance for conditions with only spatial or only temporal predictability (S+T+ versus S+T-, \textit{t}(58) = -1.60, \textit{p} = 0.116; S-T- versus S+T+ versus S-T+, \textit{t}(58) = -1.77, \textit{p} = 0.082).

% RT
% Assignment: F(1, 57) = 0.713, p = 0.402
% Spatial: F(1, 57) = 10.99, p = 0.00159 *
% Temporal: F(1, 57) = 0.527, p = 0.471
% Int: F(1, 57) = 1.209, p = 0.2761 

Subjects were also slower to respond when the training period was spatially predictable (\textit{F}(1, 57) = 10.99, \textit{p} = 0.002). A similar effect for temporal predictability failed to reach significance (\textit{F}(1, 57) = 0.53, \textit{p} = 0.471), nor did the interaction between spatial and temporal predictability (\textit{F}(1, 57) = 1.21, \textit{p} = 0.276). 

Effects were highly variable across target objects (Figure \ref{fig:bpleast_behave_obj}). Target-condition assignment did not significantly affect accuracy or reaction times (both \textit{p}'s $>$ 0.05), but often interacted with predictability effects and their interactions. One reason for this variability regards the orthographic projection used to render the objects. Previous research has indicated that recognition accuracy fluctuates as a function of how well the two-dimensional projection of an object captures its full three-dimensional structure \cite{BalasSinha09b}. For example, when there is a large amount of foreshortening in the projection, it could be difficult to infer the length of line segments that compose the object, impairing recognition. These degenerate projections are generally diametrically opposed on the object. 

% oh, behave, pt 2: objs
\begin{figure}[h!]
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=80mm]{figs/chap_bpleast/results_accuracy_obj.pdf} &
\includegraphics[width=80mm]{figs/chap_bpleast/results_rt_obj.pdf} \\
\end{tabular}
\end{center}
\caption{Behavioral measures for each target object}{Accuracy and reaction times for each target object. Horizontal axes denote target and colors predictability during the training period. Error bars depict between-subjects standard error.}
\label{fig:bpleast_behave_obj}
\end{figure}

Accuracy was computed as a function of viewing angle for each target object to investigate whether it interacted with predictability during the training period (Figure \ref{fig:bpleast_behave_rot}). Test trials during which distractor objects were presented were excluded from this analysis since there is no consistent relationship between the targets and distractors across viewing angles and thus they would only contribute noise. With the exception of target object 1, all objects indicated fluctuations in accuracy as a function of viewing angle with two diametrically opposed degenerate views. The most consistent differences in accuracy between training conditions appeared to be localized to the troughs of the accuracy function, corresponding to these degenerate views.

% oh, behave, pt 3: rots
\begin{figure}[h!]
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=80mm]{figs/chap_bpleast/results_accuracy_rot_obj1.pdf} &
\includegraphics[width=80mm]{figs/chap_bpleast/results_accuracy_rot_obj2.pdf} \\
\includegraphics[width=80mm]{figs/chap_bpleast/results_accuracy_rot_obj3.pdf} &
\includegraphics[width=80mm]{figs/chap_bpleast/results_accuracy_rot_obj4_montage.pdf} \\
\end{tabular}
\end{center}
\caption{Accuracy as a function of viewing angle for each target object}{Target accuracy at each viewing angle presented during the test periods. Horizontal axes denote viewing angle and colors predictability during the training period. Error bars depict between-subjects standard error. Diametrically opposed foreshortened views and one canonical view are shown for target object 4.}
\label{fig:bpleast_behave_rot}
\end{figure}

Standard statistical tests did not have enough power to detect differences between conditions for degenerate views because each data point corresponded to only four trials per subject. To address this design limitation, a bootstrapping method was used to resample the available data in these cases. The accuracy function over viewing angles was collapsed across conditions and the two minima associated with degenerate views were identified for each object. For target object 1, this corresponded to angles $\theta$ = \{24$^\circ$, 312$^\circ$\}, object 2: $\theta$ = \{48$^\circ$, 240$^\circ$\} object 3: $\theta$ = \{144$^\circ$, 312$^\circ$\}, and object 4 $\theta$ = \{24$^\circ$, 192$^\circ$\}. Accuracy for the completely unpredictable (S-T-) and combined spatial and temporal predictability (S+T+) conditions during training was averaged at these viewing angles and resampled with replacement from the 59 subjects for 10000 iterations. This produced distributions for degenerate view accuracy for each object (Figure \ref{fig:bpleast_behave_bootstrap}). Accuracy for was lower for degenerate views for the combined spatial and temporal predictability condition for all target objects except target 1, which didn't exhibit the patterned accuracy function that other targets did. The S+T+/S-T- difference in accuracy for degenerate views was significant at the 90\% alpha level (i.e., the confidence interval of the difference between means did not include zero) for all target objects except target 1.

% oh, behave, pt 4: bootstraps
\begin{figure}[h!]
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=160mm]{figs/chap_bpleast/results_bootstrap_montage.pdf}
\end{tabular}
\end{center}
\caption{Bootstrapped accuracy for degenerate views}{Average target accuracy for degenerate views resampled with replacement from the 59 subjects for 10000 iterations. Viewing angle for averaging is noted for each target object. Asterisks denote significant differences based on 90\% confidence intervals.}
\label{fig:bpleast_behave_bootstrap}
\end{figure}

\section{Discussion}\subsection{Summary of results}
The work described in this chapter investigated how predictability biased learned representations of novel objects. The experimental paradigm used to address this question involved training participants to recognize novel objects while manipulating their spatial and temporal predictability. Somewhat surprisingly, accuracy was lowest when stimuli were learned in a combined spatially and temporally predictable context and highest when learned in a completely unpredictable context. Reaction times were also slower when objects were learned with spatial predictability.

Behavioral measures were highly variable across objects. There was some indication that differences between predictability conditions during training were driven primarily by degenerate viewing angles caused by three-dimensional foreshortening in the objects used. A foreshortening model has previously accounted for the principal component of variability in recognition accuracy for the same ``paper clip'' objects used here \cite{BalasSinha09b}, but only in a combined spatial and temporal predictability learning context.

\subsection{A behavioral disadvantage for spatial prediction during object learning}

Based on previous research, it is unclear whether spatial predictability is used by the brain during object learning and whether it is actually advantageous. Initial investigations described in \incite{LawsonHumphreysWatson94} identified the expected increase in recognition accuracy for spatially predictable sequences. The foreshortening model advanced in \cite{BalasSinha09b} was also improved by incorporating spatial information (e.g., the first- and second-order derivatives of the foreshortening function over object views). Furthermore, a number of computational models, including LeabraTI (Chapter \ref{chap:leabrati}), use learning rules that incorporate associations between subsequent inputs to learn object representations \cite{Foldiak91,StringerPerryRollsEtAl06,WallisBaddeley97,IsikLeiboPoggio12}.

Experiments described in \incite{HarmanHumphrey99} failed to find any positive or negative effects of spatial predictability on accuracy. They did, however, increase in reaction time for objects learned in a spatially predictable context, similar to the one reported here. One possible reason for the slowing of reaction times for objects learned with spatial predictability is that less attention is necessary in these conditions. A constantly changing sequence of views might require more attentive processing to encode whereas the relatively low amount of change between views in spatially predictable sequences is less ``surprising'' and some views might be overlooked during encoding. However, there was some indication that the adverse effect of spatial predictability was driven primarily by the degenerate views of the stimuli used in the present work. A more focused experiment would be necessary to explicitly test the hypothesis impaired for degenerate views learned in a spatially predictable context and relatively intact for canonical views, but the interpretation of this hypothesis if confirmed is still discussed here. % to motivate simulations described in other chapters and future work.

% * degraaf -- lower hit rate for 10 Hz

\subsection{Spatiotemporal prediction biases development of invariance}

The theory advanced here is that spatially predictable sequences promote learning of view-invariant object representations, whereas spatially unpredictable sequences promote learning view-dependent representations. 

% core theory

% Li & Dicarlo spatiotemporal learning
% * CoxMeierOerteltEtAl05
% * LiDiCarlo08,LiDiCarlo10
% * MeyerOlson11 
% * WallisBackusLangerEtAl09,Wallis96,Wallis02,WallisBulthoff01,WallisBulthoff99 (review)
% * Not about invariance per se -- SakaiMiyashita91,MeyerOlson11 -- but related to why static views might not be enough
% * Perret et al -- view dependent cells faster to respond than view invariance cells

% previous research on spatial ordering during learning
% * \cite{LawsonHumphreysWatson94} -- familiar objects, S+ acc advantage
%		* but S+ for two frames in a row but random on average better than globally coherent? (see Exp 3 LS vs GC)
% 		* ``the identification benefit was as large for globally incoherent sequences with locally similar views (LS sequences compared with GC sequences, experiment
%		3) as for sequences that were both globally coherent and locally similar (structured sequences compared with random sequences, experiment 2);''
% 		* ``The data therefore suggest that the activation of highly view-specific representations is necessary to generate the structured sequence benefit. The benefit is
%			eliminated if consecutive views are rotated by more than 30¡ in depth. It is not changes in the visibility of parts, but metric changes in the position of features in
%			the image as objects are rotated, that minimise the combined activation of common representations and so eliminate the structured sequence benefit.''
% * ThorntonKourtzi02 -- faces -- dynamic primes needed to properly activate learned memory trace -- single image is not enough
% * * See also BalasSinha09,VuongTarr,ChuangVuongBulthoff12

\bibliographystyle{apa}
\bibliography{ccnlab}
\end{document}